{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "import re\n",
    "import math\n",
    "import random\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset gsm8k (C:/Users/29pie/.cache/huggingface/datasets/gsm8k/main/1.1.0/37bfb08b1d4fcbb01f06b03d9e1ef5f1fcbd4d3af3d08842c50d7305091285ba)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5be09e951cb455288eab0315a27f7ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?',\n",
       " 'answer': 'The discount price of one glass is 60/100 * 5 = $<<60/100*5=3>>3.\\nIf every second glass is cheaper, that means Kylar is going to buy 16 / 2 = <<16/2=8>>8 cheaper glasses.\\nSo for the cheaper glasses, Kylar is going to pay 8 * 3 = $<<8*3=24>>24.\\nAnd for the regular-priced glasses, Kylar will pay 8 * 5 = $<<8*5=40>>40.\\nSo in total Kylar needs to pay 24 + 40 = $<<24+40=64>>64 for the glasses he wants to buy.\\n#### 64'}"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "dataset[\"test\"][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_name = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "model_basename = \"./llama-2-7b-chat.Q5_K_M.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'llama_cpp'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mhuggingface_hub\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hf_hub_download\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mllama_cpp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Llama\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'llama_cpp'"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from llama_cpp import Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tokenizers in c:\\users\\29pie\\anaconda3\\lib\\site-packages (0.13.3)\n"
     ]
    }
   ],
   "source": [
    "!pip install tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model_path \u001b[38;5;241m=\u001b[39m hf_hub_download(repo_id\u001b[38;5;241m=\u001b[39mmodel_name, filename\u001b[38;5;241m=\u001b[39mmodel_basename)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model_name' is not defined"
     ]
    }
   ],
   "source": [
    "model_path = hf_hub_download(repo_id=model_name, filename=model_basename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "output = query({\n",
    "    \"inputs\": \"if i have 2 apples and someone gives me 2 apples, how many apples do i have? \",\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': 'if i have 2 apples and someone gives me 2 apples, how many apples do i have? 4\\n\\nhow many apples do i have if i have 2 apples and someone takes 2 apples from me? 0\\n\\nhow many apples do i have if i have 2 apples and someone gives me 2 apples and then takes 2 apples from me? 0\\n\\nhow many apples do i have if i have 2 apples and someone gives me 2 apples and then gives me 2 more apples? '}]\n"
     ]
    }
   ],
   "source": [
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "if i have 2 apples and someone gives me 2 apples, how many apples do i have? 4\n"
     ]
    }
   ],
   "source": [
    "print(output[0]['generated_text'].split('\\n\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_test_object(dataset):\n",
    "    if \"test\" in dataset and len(dataset[\"test\"]) > 0:\n",
    "        random_index = random.randint(0, len(dataset[\"test\"]) - 1)\n",
    "        test_object = dataset[\"test\"][random_index]\n",
    "        \n",
    "        # Assuming each object in the dataset has 'question' and 'answer' keys\n",
    "        question = test_object.get(\"question\", \"No question found\")\n",
    "        answer = test_object.get(\"answer\", \"No answer found\")\n",
    "        \n",
    "        return question, answer\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_mutation(csv_file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, header=None, encoding='utf-8', delimiter='.') \n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file_path, header=None, encoding='ISO-8859-1', delimiter='.') \n",
    "\n",
    "    random_prompt = random.choice(df[1].tolist())\n",
    "    return random_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_random_mutation_txt(txt_file_path):\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove any leading/trailing whitespace and filter out empty lines\n",
    "    prompts = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    if prompts:\n",
    "        return random.choice(prompts)\n",
    "    else:\n",
    "        return \"No mutation prompts found.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# write the task description here:\n",
    "task_description = \"Generate an instruction on how to solve the problem, based on the given question \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "task_description1 = \"Generate an instruction, or advice on how to solve a problem\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Hide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# getting mutated prompts\n",
    "question, answer = get_random_test_object(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jackie is trying to decide whether to do her taxes herself or hire an accountant. If she does the taxes herself, she'll be able to do 3 fewer hours of freelance work, losing $35/hour in missed income. The accountant charges $90. How much more money will she have if she hires the accountant?\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First find the total lost revenue if Jackie does her taxes herself: $35/hour * 3 hours = $<<35*3=105>>105\n",
      "Then subtract the accountant's charge to find how much money Janet saves: $105 - $90 = $<<105-90=15>>15\n",
      "#### 15\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Instruction Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_answer(answer, output):\n",
    "    if not (isinstance(answer, str) and isinstance(output, str)):\n",
    "        raise TypeError(\"Must be of type str\")\n",
    "    if re.search(\"\\s\" + answer +\"\\s*\", output):\n",
    "        # print(\"answer is correct\")\n",
    "        return 1\n",
    "    # print(\"answer is wrong\")\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_logic(answer):\n",
    "    return re.findall(\"<<(.*?)>>\", answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tokenise_logic(logic):\n",
    "    tokenised_logic = []\n",
    "    operators = \"+-/*=\"\n",
    "    current_token = \"\"\n",
    "    \n",
    "    for c in logic:\n",
    "        if c in operators:\n",
    "            tokenised_logic.append(current_token)\n",
    "            tokenised_logic.append(c)\n",
    "            current_token = \"\"\n",
    "        elif c == '.' or c.isnumeric():\n",
    "            current_token += c\n",
    "    tokenised_logic.append(current_token)\n",
    "    \n",
    "    return tokenised_logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_logic_sentence(line, logic_tokens):\n",
    "    i = 0\n",
    "\n",
    "    for token in logic_tokens:\n",
    "        if token in line:\n",
    "            i += 1\n",
    "\n",
    "    if len(logic_tokens) == i:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import requests\n",
    "\n",
    "class llamathwiz:\n",
    "    def __init__(self):\n",
    "        # self.tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "        # self.model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
    "        self.gen = 0\n",
    "        # self.llm = Llama(\n",
    "        #     model_path=model_path,\n",
    "        #     n_threads=2,\n",
    "        #     n_batch=512,\n",
    "        #     n_gpu_layers=32\n",
    "        #     )\n",
    "        self.API_URL = \"https://api-inference.huggingface.co/models/openchat/openchat_3.5\"\n",
    "        self.headers = {\"Authorization\": \"\"}\n",
    "\n",
    "    def query(self, payload):\n",
    "        message = ({\n",
    "            \"inputs\": f'{payload}. Give your answer in the format ANSWER: __',})\n",
    "        self.response = requests.post(self.API_URL, headers=self.headers, json=message)\n",
    "        \n",
    "        for answer in self.response.json()[0]['generated_text'].split('\\n\\n'):\n",
    "            if not answer.__contains__('ANSWER: __'):\n",
    "                if 'ANSWER:' in answer or 'A:' in answer:\n",
    "                    return answer\n",
    "            \n",
    "        if len(self.response.json()[0]['generated_text'].split('\\n\\n')) > 1:\n",
    "            return self.response.json()[0]['generated_text'].split('\\n\\n')[1]\n",
    "        return self.response.json()[0]['generated_text'].split('\\n\\n')[0]\n",
    "    \n",
    "    def query1(self, payload):\n",
    "        message = ({\n",
    "            \"inputs\": f'{payload}',})\n",
    "        self.response = requests.post(self.API_URL, headers=self.headers, json=message)\n",
    "        \n",
    "        for answer in self.response.json()[0]['generated_text'].split('\\n\\n'):\n",
    "                if 'ANSWER:' in answer:\n",
    "                    return answer\n",
    "            \n",
    "        return self.response.json()[0]['generated_text'].split('\\n\\n')[1]\n",
    "        \n",
    "    def generate_instructions1(self, question, task_description, max_tokens=100):\n",
    "        formatted_input = f\"{task_description} {question}\"\n",
    "        output = self.query(formatted_input)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def generate_instructions1(self, num_instructions, max_tokens=1000, temperature=1.5):\n",
    "        instructions = []\n",
    "        for num in range(num_instructions):\n",
    "            formatted_input = \"Generate an instruction on how to solve a simple math problem without the need of a question\"\n",
    "            generated_text = self.query(formatted_input)\n",
    "            instructions.append(generated_text)\n",
    "\n",
    "        return instructions\n",
    "\n",
    "#         formatted_input = f\"Generate {num_instructions} separate instructions on how to solve a simple math problem without the need of a question.\"\n",
    "#         generated_text = self.llm(formatted_input, max_tokens=2048)\n",
    "\n",
    "#         return generated_text\n",
    "\n",
    "    def process_with_llm(self, prompt):\n",
    "        return self.query(prompt)\n",
    "    \n",
    "    def apply_mutation(self, instruction, mutation_prompt):\n",
    "        # Example mutation - this can be customized based on your mutation logic\n",
    "        return self.query1(f\"{instruction} \\n Rewrite this and apply the following mutation. Mutation: {mutation_prompt}\")\n",
    "    \n",
    "    def fitness(self, database_answer, output):\n",
    "        final_answer = database_answer.split()[-1]\n",
    "\n",
    "        score = 0\n",
    "\n",
    "        logic = get_logic(database_answer)\n",
    "\n",
    "        tokenised_logic_sentences = []\n",
    "\n",
    "        for l in logic:\n",
    "            tokenised_logic_sentences.append(tokenise_logic(l))\n",
    "\n",
    "        total_score = 1 + len(tokenised_logic_sentences)\n",
    "\n",
    "        for line in output.split('\\n'):\n",
    "            for sentence in tokenised_logic_sentences:\n",
    "                if check_logic_sentence(line, sentence):\n",
    "                    tokenised_logic_sentences.pop(tokenised_logic_sentences.index(sentence))\n",
    "                    score += 1\n",
    "                    break\n",
    "\n",
    "        num_answer = 'a'\n",
    "\n",
    "        score += check_answer(final_answer, output)\n",
    "        return score/total_score\n",
    "    \n",
    "    def iteration_first(self, num_instructions=10):\n",
    "        self.gen = 1\n",
    "        self.generated_prompts = []\n",
    "        self.generated_answers = []\n",
    "        self.iteration_questions = []\n",
    "        self.scores = []\n",
    "        \n",
    "        mutation_prompt = get_random_mutation_txt(\"./mutation_prompts.txt\")\n",
    "        for _ in range(num_instructions):\n",
    "            question, database_answer = get_random_test_object(dataset)  # Fetch a random question from your dataset\n",
    "            self.iteration_questions.append((question, database_answer))\n",
    "            instruction = self.generate_instructions1(1)\n",
    "            # mutated_instruction = self.apply_mutation(instruction, mutation_prompt)\n",
    "            # self.generated_prompts.append(mutated_instruction)\n",
    "            self.generated_prompts.append(instruction)\n",
    "            processed_output = self.query(f'Q:{question} I:{instruction}') \n",
    "            self.generated_answers.append(processed_output)\n",
    "            self.scores.append(self.fitness(database_answer, processed_output))\n",
    "            \n",
    "    \n",
    "    def iteration_execute(self):\n",
    "        self.gen += 1\n",
    "        self.generated_prompts = []\n",
    "        self.generated_answers = []\n",
    "        self.iteration_questions = []\n",
    "        self.scores = []\n",
    "\n",
    "        mutation_prompt = get_random_mutation_txt(\"./mutation_prompts.txt\")\n",
    "        for instruction in self.best_instructions:\n",
    "            question, database_answer = get_random_test_object(dataset)  # Fetch a random question from your dataset\n",
    "            self.iteration_questions.append((question, database_answer))\n",
    "            mutated_instruction = self.apply_mutation(instruction, mutation_prompt)\n",
    "            self.generated_prompts.append(mutated_instruction)\n",
    "            processed_output = self.query(f'Q:{question} I:{mutated_instruction} A:')\n",
    "            self.generated_answers.append(processed_output)\n",
    "            self.scores.append(self.fitness(database_answer, processed_output))\n",
    "    \n",
    "    def iteration_prepare(self):\n",
    "        self.best_instructions = []\n",
    "        self.find_best_scores(self.scores)\n",
    "        self.best_instructions.append(self.replace_pos(self.best_instructions[0], self.best_instructions[1], ['ADV', 'ADJ', 'NOUN']))\n",
    "        self.best_instructions.append(self.replace_pos(self.best_instructions[1], self.best_instructions[0], ['ADV', 'ADJ', 'NOUN']))\n",
    "        for instr in self.generate_instructions1(4):\n",
    "            self.best_instructions.append(instr)\n",
    "            \n",
    "        for i in range(2):\n",
    "            mutation_prompt = get_random_mutation_txt(\"./mutation_prompts.txt\")\n",
    "            self.best_instructions.append(self.apply_mutation(self.best_instructions[i], mutation_prompt))\n",
    "        \n",
    "    def find_best_scores(self, scores):\n",
    "        for i in range(2):\n",
    "            maximum_val = 0\n",
    "            maximum_index = 0\n",
    "            while i < len(scores):\n",
    "                if scores[i] > maximum_val:\n",
    "                    maximum_val = scores[i]\n",
    "                    maximum_index = i\n",
    "                i += 1\n",
    "            scores.pop(maximum_index)\n",
    "            print(self.generated_prompts[maximum_index])\n",
    "            self.best_instructions.append(self.generated_prompts.pop(maximum_index))\n",
    "            \n",
    "    def replace_pos(self, string1, string2, POS):\n",
    "\n",
    "        \"\"\"\n",
    "        Replace tokens of specific parts of speech (POS) in string1 with corresponding\n",
    "        tokens of the same POS from string2.\n",
    "\n",
    "        Parameters:\n",
    "        - string1 (str): The input string where certain POS will be replaced.\n",
    "        - string2 (str): The reference string from which POS replacements will be taken.\n",
    "        - POS (list): A list of POS tags to identify which tokens to replace in string1.\n",
    "\n",
    "        Returns:\n",
    "        - str: The modified string with replacements of tokens based on specified POS tags.\n",
    "\n",
    "        Possible POS:\n",
    "        \"ADJ\": \"adjective\",\n",
    "        \"ADP\": \"adposition\",\n",
    "        \"ADV\": \"adverb\",\n",
    "        \"AUX\": \"auxiliary\",\n",
    "        \"CONJ\": \"conjunction\",\n",
    "        \"CCONJ\": \"coordinating conjunction\",\n",
    "        \"DET\": \"determiner\",\n",
    "        \"INTJ\": \"interjection\",\n",
    "        \"NOUN\": \"noun\",\n",
    "        \"NUM\": \"numeral\",\n",
    "        \"PART\": \"particle\",\n",
    "        \"PRON\": \"pronoun\",\n",
    "        \"PROPN\": \"proper noun\",\n",
    "        \"PUNCT\": \"punctuation\",\n",
    "        \"SCONJ\": \"subordinating conjunction\",\n",
    "        \"SYM\": \"symbol\",\n",
    "        \"VERB\": \"verb\".\n",
    "        \"\"\"\n",
    "        nlp = spacy.load(\"en_core_web_sm\")\n",
    "        doc1 = nlp(string1)\n",
    "        doc2 = nlp(string2)\n",
    "\n",
    "        new_tokens = []\n",
    "\n",
    "        for token1 in doc1:\n",
    "            if token1.pos_ in POS:\n",
    "                #generating a list of all matching tokens\n",
    "                matching_tokens = [token2.text for token2 in doc2 if token2.pos_ == token1.pos_]\n",
    "                #use a random token if possible, or else use the same\n",
    "                if matching_tokens:\n",
    "                    new_token = random.choice(matching_tokens)\n",
    "                else:\n",
    "                    new_token = token1.text\n",
    "            else:\n",
    "                new_token = token1.text\n",
    "            new_tokens.append(new_token)\n",
    "\n",
    "        # Join the modified tokens to form the final string\n",
    "        result = ' '.join(new_tokens)\n",
    "        return result\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ai = llamathwiz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following is a math problem:\n"
     ]
    }
   ],
   "source": [
    "print(ai.apply_mutation(\"solve this math problem.\", \"make this instruction more precise\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ai.iteration_first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.0, 0.25, 0.0, 0.0, 0.2, 0.0, 0.0, 0.0, 0.0]"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# self.generated_prompts = []\n",
    "# self.generated_answers = []\n",
    "# self.iteration_questions = []\n",
    "# self.scores = []\n",
    "\n",
    "ai.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Answer: 10\\nStep-by-step solution\\nFirst, we need to figure out how many square inches Milo can cover with two bags of glass chips. We know that each bag has 72 chips, and it takes 12 chips to cover one square inch. So, two bags will have 72 x 2 = 144 chips. With 144 chips, Milo can cover 144 / 1'"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.generated_answers[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Milo has two bags of glass chips, so he has 72 * 2 = <<72*2=144>>144 chips.\\nWith those, he can make 144 / 12 = <<144/12=12>>12 square inches of mosaic.\\nThus, Milo can make his 3 inch tall mosaic 12 / 3 = <<12/3=4>>4 inches long.\\n#### 4'"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.iteration_questions[2][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: 10\n",
      "ANSWER: 10\n"
     ]
    }
   ],
   "source": [
    "ai.iteration_prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ai.best_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      19.04 ms /    31 runs   (    0.61 ms per token,  1628.24 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5046.00 ms /   358 tokens (   14.09 ms per token,    70.95 tokens per second)\n",
      "llama_print_timings:        eval time =    3271.80 ms /    30 runs   (  109.06 ms per token,     9.17 tokens per second)\n",
      "llama_print_timings:       total time =    8687.36 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      96.94 ms /   128 runs   (    0.76 ms per token,  1320.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3396.64 ms /   233 tokens (   14.58 ms per token,    68.60 tokens per second)\n",
      "llama_print_timings:        eval time =   14213.54 ms /   127 runs   (  111.92 ms per token,     8.94 tokens per second)\n",
      "llama_print_timings:       total time =   19486.34 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      87.47 ms /   128 runs   (    0.68 ms per token,  1463.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4885.10 ms /   335 tokens (   14.58 ms per token,    68.58 tokens per second)\n",
      "llama_print_timings:        eval time =   14985.01 ms /   127 runs   (  117.99 ms per token,     8.48 tokens per second)\n",
      "llama_print_timings:       total time =   21810.68 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      14.68 ms /    27 runs   (    0.54 ms per token,  1838.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3014.92 ms /   209 tokens (   14.43 ms per token,    69.32 tokens per second)\n",
      "llama_print_timings:        eval time =    2831.09 ms /    26 runs   (  108.89 ms per token,     9.18 tokens per second)\n",
      "llama_print_timings:       total time =    6127.84 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      52.72 ms /    81 runs   (    0.65 ms per token,  1536.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2106.70 ms /   133 tokens (   15.84 ms per token,    63.13 tokens per second)\n",
      "llama_print_timings:        eval time =    8426.98 ms /    80 runs   (  105.34 ms per token,     9.49 tokens per second)\n",
      "llama_print_timings:       total time =   11456.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      86.82 ms /   128 runs   (    0.68 ms per token,  1474.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3886.33 ms /   269 tokens (   14.45 ms per token,    69.22 tokens per second)\n",
      "llama_print_timings:        eval time =   13739.10 ms /   127 runs   (  108.18 ms per token,     9.24 tokens per second)\n",
      "llama_print_timings:       total time =   19038.79 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      42.99 ms /    65 runs   (    0.66 ms per token,  1512.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3872.76 ms /   273 tokens (   14.19 ms per token,    70.49 tokens per second)\n",
      "llama_print_timings:        eval time =    6954.61 ms /    64 runs   (  108.67 ms per token,     9.20 tokens per second)\n",
      "llama_print_timings:       total time =   11411.71 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      85.10 ms /   128 runs   (    0.66 ms per token,  1504.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3020.54 ms /   215 tokens (   14.05 ms per token,    71.18 tokens per second)\n",
      "llama_print_timings:        eval time =   13580.90 ms /   127 runs   (  106.94 ms per token,     9.35 tokens per second)\n",
      "llama_print_timings:       total time =   17865.97 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      56.63 ms /    87 runs   (    0.65 ms per token,  1536.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6135.19 ms /   422 tokens (   14.54 ms per token,    68.78 tokens per second)\n",
      "llama_print_timings:        eval time =    9220.72 ms /    86 runs   (  107.22 ms per token,     9.33 tokens per second)\n",
      "llama_print_timings:       total time =   16268.62 ms\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    8529.41 ms\n",
      "llama_print_timings:      sample time =      84.54 ms /   128 runs   (    0.66 ms per token,  1514.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3557.14 ms /   239 tokens (   14.88 ms per token,    67.19 tokens per second)\n",
      "llama_print_timings:        eval time =   13749.43 ms /   127 runs   (  108.26 ms per token,     9.24 tokens per second)\n",
      "llama_print_timings:       total time =   18719.88 ms\n"
     ]
    }
   ],
   "source": [
    "ai.iteration_execute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Encourage Creativity: For creative writing prompts like ’Write a story about X,’ an improved version could be, ’Write a fantasy story about X set in a world where Y is possible.’  mark.\\nFor example: To find 3x, multiply 3 by x.\\n\\nSolution:\\nTo find 2x, multiply 2 by x.'"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ai.generated_prompts[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai.iteration_first()\n",
    "for i in range(49):\n",
    "    ai.iteration_prepare()\n",
    "    ai.iteration_execute()\n",
    "    \n",
    "ai.score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Llama.__call__() got an unexpected keyword argument 'num_return_sequences'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m instructions \u001b[38;5;241m=\u001b[39m ai\u001b[38;5;241m.\u001b[39mgenerate_instructions1(\u001b[38;5;241m3\u001b[39m)\n",
      "Cell \u001b[0;32mIn[53], line 32\u001b[0m, in \u001b[0;36mllamathwiz.generate_instructions1\u001b[0;34m(self, num_instructions, max_tokens, temperature)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_instructions1\u001b[39m(\u001b[38;5;28mself\u001b[39m, num_instructions, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m#         instructions = []\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#         for num in range(num_instructions):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m \n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#         return instructions\u001b[39;00m\n\u001b[1;32m     31\u001b[0m         formatted_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGenerate instructions on how to solve a simple math problem without the need of a question.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 32\u001b[0m         generated_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm(formatted_input, max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2048\u001b[39m, num_return_sequences\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m generated_text\n",
      "\u001b[0;31mTypeError\u001b[0m: Llama.__call__() got an unexpected keyword argument 'num_return_sequences'"
     ]
    }
   ],
   "source": [
    "instructions = ai.generate_instructions1(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Solution: To determine how many times Peter can go to the movies, we need to divide his total budget by the cost of one movie ticket and popcorn. Since the cost of one movie ticket is $7 and the cost of one bag of popcorn is also $7, we can calculate the number of movies he can afford as follows:\n",
      "42 dollars / 7 dollars = 6 times\n",
      "Therefore, Peter can go to the movies 6 times with the budget he has for the week.\n"
     ]
    }
   ],
   "source": [
    "print(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " step by step.\n",
      "\n",
      "Solution:\n",
      "Step 1: Read the Problem Carefully\n",
      "The first step in solving any math problem is to read it carefully. Make sure you understand what is being asked and what information is given. Pay attention to any symbols, units, or keywords that are used in the problem.\n",
      "\n",
      "Step 2: Identify the Type of Problem\n",
      "Once you have read the problem, try to identify the type of problem you are dealing with. Is it a multiplication problem, division problem, addition problem, subtraction problem, etc.? This will help you determine the best strategy for solving the\n",
      "\n",
      "\n",
      "Please provide me with an example of a math problem that you would like me to generate instructions for, and I will do my best to create step-by-step instructions on how to solve it.\n",
      " step by step:\n",
      "\n",
      "Step 1: Read the Problem Carefully\n",
      "The first step in solving any math problem is to read it carefully. Make sure you understand what is being asked and what information you need to provide in your solution. Pay attention to any given constraints or limitations that may impact your answer.\n",
      "Step 2: Identify the Key Concepts\n",
      "Once you have read the problem, try to identify the key concepts involved. What mathematical operations or principles are being used? Are there any patterns or relationships that can help guide your solution? By understanding the underlying concepts, you will be better able to approach the problem\n"
     ]
    }
   ],
   "source": [
    "for instr in instructions:\n",
    "    print(instr + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " using the concept of \"similarity\"\n",
      "\n",
      "Answer: Sure! Here are three instructions on how to solve a math problem using the concept of similarity:\n",
      "\n",
      "Instruction 1: Use Similar Shapes to Find the Missing Side Length\n",
      "\n",
      "Problem: In the figure below, two similar triangles have been drawn. The length of one side of each triangle is given, as well as the ratio of their corresponding sides. Find the missing side length of the second triangle using the concept of similarity.\n",
      "\n",
      "Solution: To solve this problem, we can use the fact that similar triangles have proportional side lengths. Since\n"
     ]
    }
   ],
   "source": [
    "print(instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nStep-by-step guide for solving a simple math problem:\\n\\nStep 1: Identify the problem type\\nThe first step in solving a math problem is to identify the type of problem you are dealing with. Is it an addition, subtraction, multiplication, or division problem? Make sure you understand the operation required to solve the problem.\\n\\nStep 2: Gather information and simplify the problem\\nOnce you have identified the type of problem, gather all the necessary information from the problem statement. Simplify the problem by removing any unnecessary factors or variables. This will make it easier to solve the problem.\\n\\nStep 3: Solve the problem using the appropriate operation\\nNow that you have gathered all the necessary information and simplified the problem, it's time to solve it! Use the appropriate operation (addition, subtraction, multiplication, or division) to find the solution. Be sure to perform the operation correctly and accurately.\\n\\nConclusion: Check your answer and simplify it if necessary\\nAfter you have solved the problem, check your answer to make sure it is correct. If necessary, simplify your answer to make it easier to understand or to compare with other answers. By following these steps, you will be able to solve simple math problems with ease and accuracy.\""
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instructions[\"choices\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def fitness(database_answer, output):\n",
    "    final_answer = database_answer.split()[-1]\n",
    "\n",
    "    score = 0\n",
    "\n",
    "    logic = get_logic(database_answer)\n",
    "\n",
    "    tokenised_logic_sentences = []\n",
    "\n",
    "    for l in logic:\n",
    "        tokenised_logic_sentences.append(tokenise_logic(l))\n",
    "    print(tokenised_logic_sentences)\n",
    "\n",
    "    total_score = 1 + len(tokenised_logic_sentences)\n",
    "\n",
    "    for line in output.split(\".\"):\n",
    "        for sentence in tokenised_logic_sentences:\n",
    "            if check_logic_sentence(line, sentence):\n",
    "                tokenised_logic_sentences.pop(tokenised_logic_sentences.index(sentence))\n",
    "                score += 1\n",
    "                break\n",
    "    last_line = line\n",
    "    print(tokenised_logic_sentences)\n",
    "\n",
    "    num_answer = 'a'\n",
    "\n",
    "    for word in last_line.split(\" \"):\n",
    "        try:\n",
    "            num_answer = str(int(word))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    score += check_answer(final_answer, num_answer)\n",
    "    return score/total_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['3', '+', '2', '=', '5'], ['100', '/', '5', '=', '20'], ['20', '*', '3', '=', '60'], ['10', '=', '10'], ['60', '-', '10', '=', '50']]\n",
      "[['100', '/', '5', '=', '20'], ['20', '*', '3', '=', '60'], ['10', '=', '10'], ['60', '-', '10', '=', '50']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.16666666666666666"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fitness('The $100 was divided into 3 + 2 = <<3+2=5>>5 parts.\\nSo each part is $100/5 = $<<100/5=20>>20.\\nSo Gerald received $20 x 3 = $<<20*3=60>>60.\\nGerald bought a book at $<<10=10>>10.\\nTherefore, Gerald was left with $60 - $10 = $<<60-10=50>>50.\\n#### 50',\n",
    "\"2 + 3 = 5. Easy peasy, right? Now let's see how many apps Travis has on his tablet now:\\nA:Travis has 61 - 9 = 52 apps on his tablet after deleting 9 apps he didn't use anymore. Then, he downloaded 18 more apps, so he has 52 + 18 = 70 apps on his tablet now!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
