{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import pandas as pd\n",
    "import random\n",
    "import json\n",
    "from datasets import load_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flan-t5-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "t5_model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MetaMath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-math/MetaMath-Llemma-7B\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-math/MetaMath-Llemma-7B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting to dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"gsm8k\", \"main\")\n",
    "dataset[\"test\"][5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get random question, answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_test_object(dataset):\n",
    "    if \"test\" in dataset and len(dataset[\"test\"]) > 0:\n",
    "        random_index = random.randint(0, len(dataset[\"test\"]) - 1)\n",
    "        test_object = dataset[\"test\"][random_index]\n",
    "        \n",
    "        # Assuming each object in the dataset has 'question' and 'answer' keys\n",
    "        question = test_object.get(\"question\", \"No question found\")\n",
    "        answer = test_object.get(\"answer\", \"No answer found\")\n",
    "        \n",
    "        return question, answer\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to get random mutation from dataset\n",
    "- CSV file data is not consistent, hence I get the mutation from the txt file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get from csv file\n",
    "def get_random_mutation(csv_file_path):\n",
    "    try:\n",
    "        df = pd.read_csv(csv_file_path, header=None, encoding='utf-8', delimiter='.') \n",
    "    except UnicodeDecodeError:\n",
    "        df = pd.read_csv(csv_file_path, header=None, encoding='ISO-8859-1', delimiter='.') \n",
    "\n",
    "    random_prompt = random.choice(df[1].tolist())\n",
    "    return random_prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get from txt file\n",
    "def get_random_mutation_txt(txt_file_path):\n",
    "    with open(txt_file_path, 'r', encoding='utf-8') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    # Remove any leading/trailing whitespace and filter out empty lines\n",
    "    prompts = [line.strip() for line in lines if line.strip()]\n",
    "    \n",
    "    if prompts:\n",
    "        return random.choice(prompts)\n",
    "    else:\n",
    "        return \"No mutation prompts found.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to generate intruction, which takes the following parameters:\n",
    "- question: Obtain from get random_test_object()\n",
    "- task_description: Description of the task we want the LLM to do\n",
    "- max_tokens: To increase max_token limit, given that different models have different limits\n",
    "- model: To enable the testing of different models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_intructions(question, task_description, max_tokens, model):\n",
    "    formatted_input = f\"{task_description} {question}\"\n",
    "    input_ids = tokenizer(formatted_input, return_tensors=\"pt\").input_ids\n",
    "    outputs = model.generate(input_ids, max_new_tokens = max_tokens)\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_instructions_MM(question, model, tokenizer, max_tokens=500):\n",
    "    # Define the prompting template\n",
    "    prompting_template = (\n",
    "        \"Below is an instruction that describes a task. \"\n",
    "        \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "        \"### Instruction:\\n{instruction}\\n\\n\"\n",
    "        \"### Response: Let's think step by step.\"\n",
    "    )\n",
    "\n",
    "    # Format the input using the template and the question\n",
    "    formatted_input = prompting_template.format(instruction=question)\n",
    "\n",
    "    # Tokenize the input\n",
    "    input_ids = tokenizer.encode(formatted_input, return_tensors=\"pt\")\n",
    "\n",
    "    # Generate output using the model\n",
    "    outputs = model.generate(input_ids, max_new_tokens=max_tokens)\n",
    "\n",
    "    # Decode the output to a human-readable format\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to process prompt via LLM\n",
    "- Takes prompt, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_with_llm(prompt, model):\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    output = model.generate(input_ids, max_length=1000)\n",
    "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the task description here:\n",
    "task_description = \"\"\"Generate an instruction on how to solve the problem, based on the given question \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting mutated prompts\n",
    "question, answer = get_random_test_object(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_instructions = 1\n",
    "generated_instructions = []\n",
    "\n",
    "for _ in range(num_instructions):  # Fetch a random question from your dataset\n",
    "    instruction = generate_instructions_2(question, model, tokenizer)\n",
    "    generated_instructions.append(instruction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(generated_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutation_prompt = get_random_mutation_txt(\"./Prompt-Engineering-OpenDI/mutation_prompts.txt\")\n",
    "print(mutation_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_mutation(instruction, mutation_prompt):\n",
    "    # Example mutation - this can be customized based on your mutation logic\n",
    "    return f\"{mutation_prompt} {instruction}\"\n",
    "\n",
    "mutated_instructions = [apply_mutation(instruction, mutation_prompt) for instruction in generated_instructions]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mutated_instructions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_outputs = [process_with_llm(mutated_instruction) for mutated_instruction in mutated_instructions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for response in processed_outputs:\n",
    "    print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"Kylar went to the store to buy glasses for his new apartment. One glass costs $5, but every second glass costs only 60% of the price. Kylar wants to buy 16 glasses. How much does he need to pay for them?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Specify the max_new_tokens parameter\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)  # Adjust the number as needed\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"What was the answer for the previus question I asked?\"\n",
    "input_ids = tokenizer(input_text, return_tensors=\"pt\").input_ids\n",
    "\n",
    "# Specify the max_new_tokens parameter\n",
    "outputs = model.generate(input_ids, max_new_tokens=50)  # Adjust the number as needed\n",
    "print(tokenizer.decode(outputs[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
