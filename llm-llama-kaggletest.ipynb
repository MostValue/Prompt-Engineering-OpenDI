{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4300,"sourceType":"modelInstanceVersion","modelInstanceId":3095}],"dockerImageVersionId":30615,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input/llama-2/pytorch/13b/1/'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-12-08T23:12:42.021420Z","iopub.execute_input":"2023-12-08T23:12:42.022075Z","iopub.status.idle":"2023-12-08T23:12:42.030702Z","shell.execute_reply.started":"2023-12-08T23:12:42.022014Z","shell.execute_reply":"2023-12-08T23:12:42.029277Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"/kaggle/input/llama-2/pytorch/13b/1/consolidated.00.pth\n/kaggle/input/llama-2/pytorch/13b/1/tokenizer_checklist.chk\n/kaggle/input/llama-2/pytorch/13b/1/Responsible-Use-Guide.pdf\n/kaggle/input/llama-2/pytorch/13b/1/params.json\n/kaggle/input/llama-2/pytorch/13b/1/README.md\n/kaggle/input/llama-2/pytorch/13b/1/USE_POLICY.md\n/kaggle/input/llama-2/pytorch/13b/1/checklist.chk\n/kaggle/input/llama-2/pytorch/13b/1/LICENSE.txt\n/kaggle/input/llama-2/pytorch/13b/1/.gitattributes\n/kaggle/input/llama-2/pytorch/13b/1/tokenizer.model\n/kaggle/input/llama-2/pytorch/13b/1/consolidated.01.pth\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Adapted from :\n\n- https://www.kaggle.com/code/lucamassaron/fine-tune-llama-2-for-sentiment-analysis","metadata":{}},{"cell_type":"code","source":"import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"   #Tells pytorch to use the first GPU\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\" #Do not parallise tokenisation (dont know why)\n\n#suppress warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T23:12:42.033647Z","iopub.execute_input":"2023-12-08T23:12:42.034346Z","iopub.status.idle":"2023-12-08T23:12:42.051826Z","shell.execute_reply.started":"2023-12-08T23:12:42.034297Z","shell.execute_reply":"2023-12-08T23:12:42.050371Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Using the same versions as the above notebook so nothing breaks\n!pip install -q -U accelerate==0.23.0 peft==0.5.0 bitsandbytes==0.41.1 transformers==4.33.1 trl==0.7.2","metadata":{"execution":{"iopub.status.busy":"2023-12-08T23:12:42.054621Z","iopub.execute_input":"2023-12-08T23:12:42.055120Z","iopub.status.idle":"2023-12-08T23:13:05.763508Z","shell.execute_reply.started":"2023-12-08T23:12:42.055082Z","shell.execute_reply":"2023-12-08T23:13:05.761840Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# All of our imports\nimport numpy as np\nimport pandas as pd\nimport os\nfrom tqdm import tqdm\nimport bitsandbytes as bnb\nimport torch\nimport torch.nn as nn\nimport transformers\nfrom datasets import Dataset\nfrom peft import LoraConfig, PeftConfig\nfrom trl import SFTTrainer\nfrom transformers import (AutoModelForCausalLM, \n                          AutoTokenizer, \n                          BitsAndBytesConfig, \n                          TrainingArguments, \n                          pipeline, \n                          logging)\nfrom sklearn.metrics import (accuracy_score, \n                             classification_report, \n                             confusion_matrix)\nfrom sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2023-12-08T23:13:05.766066Z","iopub.execute_input":"2023-12-08T23:13:05.766573Z","iopub.status.idle":"2023-12-08T23:13:05.776455Z","shell.execute_reply.started":"2023-12-08T23:13:05.766524Z","shell.execute_reply":"2023-12-08T23:13:05.775084Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model_name = \"/kaggle/input/llama-2/pytorch/13b-hf/1\"\n\npipeline = transformers.pipeline(\n    \"text-generation\",\n    model=model_name,\n    torch_dtype=torch.float16,\n    device_map=\"auto\",\n)","metadata":{"execution":{"iopub.status.busy":"2023-12-08T23:13:15.814430Z","iopub.execute_input":"2023-12-08T23:13:15.814896Z","iopub.status.idle":"2023-12-08T23:13:16.922623Z","shell.execute_reply.started":"2023-12-08T23:13:15.814829Z","shell.execute_reply":"2023-12-08T23:13:16.920729Z"},"trusted":true},"execution_count":17,"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1099\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     backends \u001b[38;5;241m=\u001b[39m [backends]\n\u001b[0;32m-> 1099\u001b[0m name \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__name__\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\n\u001b[1;32m   1101\u001b[0m \u001b[38;5;66;03m# Raise an error for users who might not realize that classes without \"TF\" are torch-only\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/importlib/__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1050\u001b[0m, in \u001b[0;36m_gcd_import\u001b[0;34m(name, package, level)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap_external>:883\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n","File \u001b[0;32m<frozen importlib._bootstrap>:241\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/falcon/configuration_falcon.py:21\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m logging\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mauto\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconfiguration_auto\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m sanitize_code_revision\n\u001b[1;32m     24\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mget_logger(\u001b[38;5;18m__name__\u001b[39m)\n","\u001b[0;31mImportError\u001b[0m: cannot import name 'sanitize_code_revision' from 'transformers.models.auto.configuration_auto' (/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py)","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m model_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/kaggle/input/llama-2/pytorch/13b-hf/1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mtransformers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat16\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mauto\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/__init__.py:788\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, use_auth_token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    786\u001b[0m     \u001b[38;5;66;03m# At that point framework might still be undetermined\u001b[39;00m\n\u001b[1;32m    787\u001b[0m     model, default_revision \u001b[38;5;241m=\u001b[39m get_default_model_and_revision(targeted_task, framework, task_options)\n\u001b[0;32m--> 788\u001b[0m     revision \u001b[38;5;241m=\u001b[39m revision \u001b[38;5;28;01mif\u001b[39;00m revision \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m default_revision\n\u001b[1;32m    789\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    790\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model was supplied, defaulted to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m and revision\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    791\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrevision\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mHUGGINGFACE_CO_RESOLVE_ENDPOINT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    792\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing a pipeline without specifying a model name and revision in production is not recommended.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    793\u001b[0m     )\n\u001b[1;32m    794\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pipelines/base.py:269\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    263\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    264\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel might be a PyTorch model (ending with `.bin`) but PyTorch is not available. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    265\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to load the model with Tensorflow.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    266\u001b[0m     )\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 269\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(model, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(model, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    271\u001b[0m         model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39meval()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:474\u001b[0m, in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    471\u001b[0m commit_hash \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    473\u001b[0m revision \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 474\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrevision\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m sanitize_code_revision(pretrained_model_name_or_path, revision, trust_remote_code)\n\u001b[1;32m    476\u001b[0m token \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtoken\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    477\u001b[0m use_auth_token \u001b[38;5;241m=\u001b[39m hub_kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_auth_token\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:683\u001b[0m, in \u001b[0;36mkeys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_mapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    682\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 683\u001b[0m         result \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(model)\n\u001b[1;32m    684\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    685\u001b[0m         result\u001b[38;5;241m.\u001b[39mappend(model)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:684\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    682\u001b[0m     if isinstance(model, (list, tuple)):\n\u001b[1;32m    683\u001b[0m         result += list(model)\n\u001b[0;32m--> 684\u001b[0m     else:\n\u001b[1;32m    685\u001b[0m         result.append(model)\n\u001b[1;32m    687\u001b[0m return result\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:680\u001b[0m, in \u001b[0;36m_load_attr_from_module\u001b[0;34m(self, model_type, attr)\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_values\u001b[39m(model_mapping):\n\u001b[0;32m--> 680\u001b[0m     result \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    681\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model \u001b[38;5;129;01min\u001b[39;00m model_mapping\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m    682\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/auto/auto_factory.py:625\u001b[0m, in \u001b[0;36mgetattribute_from_module\u001b[0;34m(module, attr)\u001b[0m\n\u001b[1;32m    616\u001b[0m     config \u001b[38;5;241m=\u001b[39m TimmBackboneConfig(\n\u001b[1;32m    617\u001b[0m         backbone\u001b[38;5;241m=\u001b[39mpretrained_model_name_or_path,\n\u001b[1;32m    618\u001b[0m         num_channels\u001b[38;5;241m=\u001b[39mnum_channels,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    621\u001b[0m         out_indices\u001b[38;5;241m=\u001b[39mout_indices,\n\u001b[1;32m    622\u001b[0m     )\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39mfrom_config(config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 625\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    626\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfrom_pretrained\u001b[39m(\u001b[38;5;28mcls\u001b[39m, pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_timm_backbone\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    628\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_load_timm_backbone_from_pretrained(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1089\u001b[0m, in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1044\u001b[0m JIEBA_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the jieba library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[1;32m   1046\u001b[0m \u001b[38;5;124mjieba`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1047\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1049\u001b[0m PEFT_IMPORT_ERROR \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1050\u001b[0m \u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m requires the peft library but it was not found in your environment. You can install it with pip: `pip install\u001b[39m\n\u001b[1;32m   1051\u001b[0m \u001b[38;5;124mpeft`. Please note that you may need to restart your runtime after installation.\u001b[39m\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m   1054\u001b[0m BACKENDS_MAPPING \u001b[38;5;241m=\u001b[39m OrderedDict(\n\u001b[1;32m   1055\u001b[0m     [\n\u001b[1;32m   1056\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbs4\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_bs4_available, BS4_IMPORT_ERROR)),\n\u001b[1;32m   1057\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdatasets\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_datasets_available, DATASETS_IMPORT_ERROR)),\n\u001b[1;32m   1058\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetectron2\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_detectron2_available, DETECTRON2_IMPORT_ERROR)),\n\u001b[1;32m   1059\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124messentia\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_essentia_available, ESSENTIA_IMPORT_ERROR)),\n\u001b[1;32m   1060\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfaiss\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_faiss_available, FAISS_IMPORT_ERROR)),\n\u001b[1;32m   1061\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflax\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_flax_available, FLAX_IMPORT_ERROR)),\n\u001b[1;32m   1062\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mftfy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ftfy_available, FTFY_IMPORT_ERROR)),\n\u001b[1;32m   1063\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpandas\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pandas_available, PANDAS_IMPORT_ERROR)),\n\u001b[1;32m   1064\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mphonemizer\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_phonemizer_available, PHONEMIZER_IMPORT_ERROR)),\n\u001b[1;32m   1065\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpretty_midi\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pretty_midi_available, PRETTY_MIDI_IMPORT_ERROR)),\n\u001b[1;32m   1066\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlibrosa\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_librosa_available, LIBROSA_IMPORT_ERROR)),\n\u001b[1;32m   1067\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprotobuf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_protobuf_available, PROTOBUF_IMPORT_ERROR)),\n\u001b[1;32m   1068\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyctcdecode\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pyctcdecode_available, PYCTCDECODE_IMPORT_ERROR)),\n\u001b[1;32m   1069\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytesseract\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytesseract_available, PYTESSERACT_IMPORT_ERROR)),\n\u001b[1;32m   1070\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msacremoses\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sacremoses_available, SACREMOSES_IMPORT_ERROR)),\n\u001b[1;32m   1071\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_quantization\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_pytorch_quantization_available, PYTORCH_QUANTIZATION_IMPORT_ERROR)),\n\u001b[1;32m   1072\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msentencepiece\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sentencepiece_available, SENTENCEPIECE_IMPORT_ERROR)),\n\u001b[1;32m   1073\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msklearn\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_sklearn_available, SKLEARN_IMPORT_ERROR)),\n\u001b[1;32m   1074\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mspeech\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_speech_available, SPEECH_IMPORT_ERROR)),\n\u001b[1;32m   1075\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_probability\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_probability_available, TENSORFLOW_PROBABILITY_IMPORT_ERROR)),\n\u001b[1;32m   1076\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tf_available, TENSORFLOW_IMPORT_ERROR)),\n\u001b[1;32m   1077\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensorflow_text\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tensorflow_text_available, TENSORFLOW_TEXT_IMPORT_ERROR)),\n\u001b[1;32m   1078\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimm\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_timm_available, TIMM_IMPORT_ERROR)),\n\u001b[1;32m   1079\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnatten\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_natten_available, NATTEN_IMPORT_ERROR)),\n\u001b[1;32m   1080\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtokenizers\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_tokenizers_available, TOKENIZERS_IMPORT_ERROR)),\n\u001b[1;32m   1081\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torch_available, PYTORCH_IMPORT_ERROR)),\n\u001b[1;32m   1082\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorchvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_torchvision_available, TORCHVISION_IMPORT_ERROR)),\n\u001b[1;32m   1083\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvision\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_vision_available, VISION_IMPORT_ERROR)),\n\u001b[1;32m   1084\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscipy\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_scipy_available, SCIPY_IMPORT_ERROR)),\n\u001b[1;32m   1085\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccelerate\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_accelerate_available, ACCELERATE_IMPORT_ERROR)),\n\u001b[1;32m   1086\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moneccl_bind_pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_ccl_available, CCL_IMPORT_ERROR)),\n\u001b[1;32m   1087\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdecord\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_decord_available, DECORD_IMPORT_ERROR)),\n\u001b[1;32m   1088\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcython\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_cython_available, CYTHON_IMPORT_ERROR)),\n\u001b[0;32m-> 1089\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjieba\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_jieba_available, JIEBA_IMPORT_ERROR)),\n\u001b[1;32m   1090\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeft\u001b[39m\u001b[38;5;124m\"\u001b[39m, (is_peft_available, PEFT_IMPORT_ERROR)),\n\u001b[1;32m   1091\u001b[0m     ]\n\u001b[1;32m   1092\u001b[0m )\n\u001b[1;32m   1095\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrequires_backends\u001b[39m(obj, backends):\n\u001b[1;32m   1096\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(backends, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/utils/import_utils.py:1101\u001b[0m, in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1097\u001b[0m     backends = [backends]\n\u001b[1;32m   1099\u001b[0m name = obj.__name__ if hasattr(obj, \"__name__\") else obj.__class__.__name__\n\u001b[0;32m-> 1101\u001b[0m # Raise an error for users who might not realize that classes without \"TF\" are torch-only\n\u001b[1;32m   1102\u001b[0m if \"torch\" in backends and \"tf\" not in backends and not is_torch_available() and is_tf_available():\n\u001b[1;32m   1103\u001b[0m     raise ImportError(PYTORCH_IMPORT_ERROR_WITH_TF.format(name))\n","\u001b[0;31mRuntimeError\u001b[0m: Failed to import transformers.models.falcon.configuration_falcon because of the following error (look up to see its traceback):\ncannot import name 'sanitize_code_revision' from 'transformers.models.auto.configuration_auto' (/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py)"],"ename":"RuntimeError","evalue":"Failed to import transformers.models.falcon.configuration_falcon because of the following error (look up to see its traceback):\ncannot import name 'sanitize_code_revision' from 'transformers.models.auto.configuration_auto' (/opt/conda/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py)","output_type":"error"}]},{"cell_type":"code","source":"model_name = \"/kaggle/input/llama-2/pytorch/13b/1\"\n\n\n#Using 16 bit floating point for computation\ncompute_dtype = getattr(torch, \"float16\")\n\n#Optimises computation for running in kaggle (not quite sure how it works)\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True, #Loading in model weights as 4 bit\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype= compute_dtype, #using 16 bit for computation\n    bnb_4bit_use_double_quant=False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    device_map=\"auto\", # Automatically distributes processing between gpu and cpu\n    quantization_config=bnb_config, \n)\n","metadata":{"execution":{"iopub.status.busy":"2023-12-08T23:13:06.097796Z","iopub.status.idle":"2023-12-08T23:13:06.098773Z","shell.execute_reply.started":"2023-12-08T23:13:06.098469Z","shell.execute_reply":"2023-12-08T23:13:06.098509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}